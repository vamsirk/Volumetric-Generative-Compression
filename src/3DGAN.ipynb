{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies not loaded, some functionality may not work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gen/Relu:0\", shape=(32, 4, 4, 4, 512), dtype=float32, device=/device:GPU:0) g1\n",
      "Tensor(\"gen/Relu_1:0\", shape=(32, 8, 8, 8, 256), dtype=float32, device=/device:GPU:0) g2\n",
      "Tensor(\"gen/Relu_2:0\", shape=(32, 16, 16, 16, 128), dtype=float32, device=/device:GPU:0) g3\n",
      "Tensor(\"gen/Relu_3:0\", shape=(32, 32, 32, 32, 64), dtype=float32, device=/device:GPU:0) g4\n",
      "Tensor(\"gen/Tanh:0\", shape=(32, 64, 64, 64, 1), dtype=float32, device=/device:GPU:0) g5\n",
      "Tensor(\"dis/Maximum:0\", shape=(32, 32, 32, 32, 64), dtype=float32, device=/device:GPU:0) d1\n",
      "Tensor(\"dis/Maximum_1:0\", shape=(32, 16, 16, 16, 128), dtype=float32, device=/device:GPU:0) d2\n",
      "Tensor(\"dis/Maximum_2:0\", shape=(32, 8, 8, 8, 256), dtype=float32, device=/device:GPU:0) d3\n",
      "Tensor(\"dis/Maximum_3:0\", shape=(32, 4, 4, 4, 512), dtype=float32, device=/device:GPU:0) d4\n",
      "Tensor(\"dis/Sigmoid:0\", shape=(32, 1, 1, 1, 1), dtype=float32, device=/device:GPU:0) d5\n",
      "Tensor(\"dis_1/Maximum:0\", shape=(32, 32, 32, 32, 64), dtype=float32, device=/device:GPU:0) d1\n",
      "Tensor(\"dis_1/Maximum_1:0\", shape=(32, 16, 16, 16, 128), dtype=float32, device=/device:GPU:0) d2\n",
      "Tensor(\"dis_1/Maximum_2:0\", shape=(32, 8, 8, 8, 256), dtype=float32, device=/device:GPU:0) d3\n",
      "Tensor(\"dis_1/Maximum_3:0\", shape=(32, 4, 4, 4, 512), dtype=float32, device=/device:GPU:0) d4\n",
      "Tensor(\"dis_1/Sigmoid:0\", shape=(32, 1, 1, 1, 1), dtype=float32, device=/device:GPU:0) d5\n",
      "Tensor(\"gen_1/Relu:0\", shape=(32, 4, 4, 4, 512), dtype=float32, device=/device:GPU:0) g1\n",
      "Tensor(\"gen_1/Relu_1:0\", shape=(32, 8, 8, 8, 256), dtype=float32, device=/device:GPU:0) g2\n",
      "Tensor(\"gen_1/Relu_2:0\", shape=(32, 16, 16, 16, 128), dtype=float32, device=/device:GPU:0) g3\n",
      "Tensor(\"gen_1/Relu_3:0\", shape=(32, 32, 32, 32, 64), dtype=float32, device=/device:GPU:0) g4\n",
      "Tensor(\"gen_1/Tanh:0\", shape=(32, 64, 64, 64, 1), dtype=float32, device=/device:GPU:0) g5\n",
      "Using chair Data\n",
      "13 11\n",
      "Discriminator Training  epoch:  0 , d_loss: 1.6751542 g_loss: 0.6491127 d_acc:  0.375\n",
      "Discriminator Training  epoch:  0 , d_loss: 1.6751542 g_loss: 0.6491127 d_acc:  0.375\n",
      "Discriminator Training  epoch:  0 , d_loss: 1.6751542 g_loss: 0.6491127 d_acc:  0.375\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Outputs of true_fn and false_fn must have the same type: int32, bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6ec2a89f951f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;31m#    ckpt = sys.argv[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mtrainGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_dummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mtrainGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_dummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6ec2a89f951f>\u001b[0m in \u001b[0;36mtrainGAN\u001b[0;34m(is_dummy, checkpoint)\u001b[0m\n\u001b[1;32m    228\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Discriminator Training '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epoch: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m', d_loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g_loss:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"d_acc: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_p_z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             instructions)\n\u001b[0;32m--> 289\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    291\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(pred, true_fn, false_fn, strict, name, fn1, fn2)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         raise ValueError(\n\u001b[1;32m   1860\u001b[0m             \u001b[0;34m\"Outputs of true_fn and false_fn must have the same type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m             (val_x.dtype.name, val_y.dtype.name))\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m     \u001b[0mmerges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_f_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_t_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Outputs of true_fn and false_fn must have the same type: int32, bool"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "import visdom\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dataIO as d\n",
    "\n",
    "from tqdm import *\n",
    "from utils import *\n",
    "\n",
    "'''\n",
    "Global Parameters\n",
    "'''\n",
    "n_epochs   = 10000\n",
    "batch_size = 32\n",
    "g_lr       = 0.0025\n",
    "d_lr       = 0.00001\n",
    "beta       = 0.5\n",
    "d_thresh   = 0.8\n",
    "z_size     = 200\n",
    "leak_value = 0.2\n",
    "cube_len   = 64\n",
    "obj_ratio  = 0.7\n",
    "obj        = 'chair' \n",
    "\n",
    "train_sample_directory = './train_sample/'\n",
    "model_directory = './models/'\n",
    "is_local = False\n",
    "\n",
    "weights = {}\n",
    "\n",
    "def generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n",
    "    \n",
    "    #with tf.device('/device:GPU:0'):\n",
    "\n",
    "        strides    = [1,2,2,2,1]\n",
    "\n",
    "        with tf.variable_scope(\"gen\", reuse=reuse):\n",
    "            z = tf.reshape(z, (batch_size, 1, 1, 1, z_size))\n",
    "            g_1 = tf.nn.conv3d_transpose(z, weights['wg1'], (batch_size,4,4,4,512), strides=[1,1,1,1,1], padding=\"VALID\")\n",
    "            g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n",
    "            g_1 = tf.nn.relu(g_1)\n",
    "\n",
    "            g_2 = tf.nn.conv3d_transpose(g_1, weights['wg2'], (batch_size,8,8,8,256), strides=strides, padding=\"SAME\")\n",
    "            g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n",
    "            g_2 = tf.nn.relu(g_2)\n",
    "\n",
    "            g_3 = tf.nn.conv3d_transpose(g_2, weights['wg3'], (batch_size,16,16,16,128), strides=strides, padding=\"SAME\")\n",
    "            g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n",
    "            g_3 = tf.nn.relu(g_3)\n",
    "\n",
    "            g_4 = tf.nn.conv3d_transpose(g_3, weights['wg4'], (batch_size,32,32,32,64), strides=strides, padding=\"SAME\")\n",
    "            g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n",
    "            g_4 = tf.nn.relu(g_4)\n",
    "\n",
    "            g_5 = tf.nn.conv3d_transpose(g_4, weights['wg5'], (batch_size,64,64,64,1), strides=strides, padding=\"SAME\")\n",
    "            # g_5 = tf.nn.sigmoid(g_5)\n",
    "            g_5 = tf.contrib.layers.batch_norm(g_5, is_training=phase_train)\n",
    "            g_5 = tf.nn.tanh(g_5)\n",
    "\n",
    "        print(g_1, 'g1')\n",
    "        print(g_2, 'g2')\n",
    "        print(g_3, 'g3')\n",
    "        print(g_4, 'g4')\n",
    "        print(g_5, 'g5')\n",
    "\n",
    "        return g_5\n",
    "\n",
    "\n",
    "def discriminator(inputs, phase_train=True, reuse=False):\n",
    "    #with tf.device('/device:GPU:0'):\n",
    "        strides    = [1,2,2,2,1]\n",
    "        with tf.variable_scope(\"dis\", reuse=reuse):\n",
    "            d_1 = tf.nn.conv3d(inputs, weights['wd1'], strides=strides, padding=\"SAME\")\n",
    "            d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n",
    "            d_1 = lrelu(d_1, leak_value)\n",
    "\n",
    "            d_2 = tf.nn.conv3d(d_1, weights['wd2'], strides=strides, padding=\"SAME\") \n",
    "            d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n",
    "            d_2 = lrelu(d_2, leak_value)\n",
    "\n",
    "            d_3 = tf.nn.conv3d(d_2, weights['wd3'], strides=strides, padding=\"SAME\")  \n",
    "            d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n",
    "            d_3 = lrelu(d_3, leak_value) \n",
    "\n",
    "            d_4 = tf.nn.conv3d(d_3, weights['wd4'], strides=strides, padding=\"SAME\")     \n",
    "            d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n",
    "            d_4 = lrelu(d_4)\n",
    "\n",
    "            d_5 = tf.nn.conv3d(d_4, weights['wd5'], strides=[1,1,1,1,1], padding=\"VALID\")     \n",
    "            d_5_no_sigmoid = d_5\n",
    "            d_5 = tf.nn.sigmoid(d_5)\n",
    "\n",
    "        print(d_1, 'd1')\n",
    "        print(d_2, 'd2')\n",
    "        print(d_3, 'd3')\n",
    "        print(d_4, 'd4')\n",
    "        print(d_5, 'd5')\n",
    "\n",
    "        return d_5, d_5_no_sigmoid\n",
    "\n",
    "def initialiseWeights():\n",
    "    #with tf.device('/device:GPU:0'):\n",
    "        global weights\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        weights['wg1'] = tf.get_variable(\"wg1\", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n",
    "        weights['wg2'] = tf.get_variable(\"wg2\", shape=[4, 4, 4, 256, 512], initializer=xavier_init)\n",
    "        weights['wg3'] = tf.get_variable(\"wg3\", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n",
    "        weights['wg4'] = tf.get_variable(\"wg4\", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n",
    "        weights['wg5'] = tf.get_variable(\"wg5\", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n",
    "\n",
    "        weights['wd1'] = tf.get_variable(\"wd1\", shape=[4, 4, 4, 1, 64], initializer=xavier_init)\n",
    "        weights['wd2'] = tf.get_variable(\"wd2\", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n",
    "        weights['wd3'] = tf.get_variable(\"wd3\", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n",
    "        weights['wd4'] = tf.get_variable(\"wd4\", shape=[4, 4, 4, 256, 512], initializer=xavier_init)    \n",
    "        weights['wd5'] = tf.get_variable(\"wd5\", shape=[4, 4, 4, 512, 1], initializer=xavier_init)    \n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "def trainGAN(is_dummy=False, checkpoint=None):\n",
    "    with tf.device('/device:GPU:0'):\n",
    "\n",
    "        weights =  initialiseWeights()\n",
    "\n",
    "        z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n",
    "        x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n",
    "\n",
    "        net_g_train = generator(z_vector, phase_train=True, reuse=False) \n",
    "\n",
    "        d_output_x, d_no_sigmoid_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n",
    "        d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n",
    "        summary_d_x_hist = tf.summary.histogram(\"d_prob_x\", d_output_x)\n",
    "\n",
    "        d_output_z, d_no_sigmoid_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n",
    "        d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n",
    "        summary_d_z_hist = tf.summary.histogram(\"d_prob_z\", d_output_z)\n",
    "\n",
    "        # Compute the discriminator accuracy\n",
    "        n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n",
    "        n_p_z = tf.reduce_sum(tf.cast(d_output_z < 0.5, tf.int32))\n",
    "        d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n",
    "\n",
    "        # Compute the discriminator and generator loss\n",
    "        # d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n",
    "        # g_loss = -tf.reduce_mean(tf.log(d_output_z))\n",
    "\n",
    "        \n",
    "        #d_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_x, labels=tf.ones_like(d_output_x)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.zeros_like(d_output_z))\n",
    "        #g_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.ones_like(d_output_z))\n",
    "    \n",
    "        #d_loss = tf.reduce_mean(d_loss)\n",
    "        #g_loss = tf.reduce_mean(g_loss)\n",
    "        \n",
    "        d_labels = tf.random_uniform( tf.shape(d_output_x), minval=0.7, maxval=1.2)\n",
    "        #g_labels = tf.random_uniform( tf.shape(d_output_x), minval=0.0, maxval=0.3)\n",
    "        # Compute the discriminator and generator loss\n",
    "        d_loss = -1*(tf.reduce_mean((d_labels*tf.log(d_output_x)) + tf.log(1-d_output_z)))\n",
    "        g_loss = -1*(tf.reduce_mean(tf.log(d_output_z)))\n",
    "\n",
    "\n",
    "        summary_d_loss = tf.summary.scalar(\"d_loss\", d_loss)\n",
    "        summary_g_loss = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "        summary_n_p_z = tf.summary.scalar(\"n_p_z\", n_p_z)\n",
    "        summary_n_p_x = tf.summary.scalar(\"n_p_x\", n_p_x)\n",
    "        summary_d_acc = tf.summary.scalar(\"d_acc\", d_acc)\n",
    "\n",
    "        net_g_test = generator(z_vector, phase_train=False, reuse=True)\n",
    "\n",
    "        para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in ['wg', 'bg', 'gen'])]\n",
    "        para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in ['wd', 'bd', 'dis'])]\n",
    "\n",
    "        # only update the weights for the discriminator network\n",
    "        optimizer_op_d = tf.train.AdamOptimizer(learning_rate=d_lr,beta1=beta).minimize(d_loss,var_list=para_d)\n",
    "        # only update the weights for the generator network\n",
    "        optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss,var_list=para_g)\n",
    "\n",
    "        saver = tf.train.Saver() \n",
    "        vis = visdom.Visdom()\n",
    "\n",
    "        config=tf.ConfigProto(allow_soft_placement=True)\n",
    "        with tf.Session(config=config) as sess:  \n",
    "\n",
    "            sess.run(tf.global_variables_initializer())        \n",
    "            if checkpoint is not None:\n",
    "                saver.restore(sess, checkpoint)        \n",
    "\n",
    "            if is_dummy:\n",
    "                volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n",
    "                print('Using Dummy Data')\n",
    "            else:\n",
    "                volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio)\n",
    "                print('Using ' + obj + ' Data')\n",
    "            volumes = volumes[...,np.newaxis].astype(np.float)\n",
    "            # volumes *= 2.0\n",
    "            # volumes -= 1.0\n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "\n",
    "                idx = np.random.randint(len(volumes), size=batch_size)\n",
    "                x = volumes[idx]\n",
    "                z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n",
    "                z = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n",
    "                # z = np.random.uniform(0, 1, size=[batch_size, z_size]).astype(np.float32)\n",
    "\n",
    "                # Update the discriminator and generator\n",
    "                d_summary_merge = tf.summary.merge([summary_d_loss,\n",
    "                                                    summary_d_x_hist, \n",
    "                                                    summary_d_z_hist,\n",
    "                                                    summary_n_p_x,\n",
    "                                                    summary_n_p_z,\n",
    "                                                    summary_d_acc])\n",
    "\n",
    "                summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n",
    "                summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z}) \n",
    "                \n",
    "                d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n",
    "                print(n_x, n_z)\n",
    "                tr = True\n",
    "                def f():\n",
    "                    coun = 3\n",
    "                    tr = False\n",
    "                    for h in range(coun):\n",
    "                        sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n",
    "                        print('Discriminator Training ', \"epoch: \",epoch,', d_loss:',discriminator_loss,'g_loss:',generator_loss, \"d_acc: \", d_accuracy)\n",
    "                    return 0\n",
    "                tf.cond(tf.divide(n_p_z, batch_size)<0.02, f, lambda: False)\n",
    "               \n",
    "                \n",
    "                if d_accuracy < d_thresh and tr:\n",
    "                    sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n",
    "                    print('Discriminator Training ', \"epoch: \",epoch,', d_loss:',discriminator_loss,'g_loss:',generator_loss, \"d_acc: \", d_accuracy)\n",
    "\n",
    "                sess.run([optimizer_op_g],feed_dict={z_vector:z})\n",
    "                print('Generator Training ', \"epoch: \",epoch,', d_loss:',discriminator_loss,'g_loss:',generator_loss, \"d_acc: \", d_accuracy)\n",
    "\n",
    "                # output generated chairs\n",
    "                if epoch % 200 == 0:\n",
    "                    g_objects = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n",
    "                    if not os.path.exists(train_sample_directory):\n",
    "                        os.makedirs(train_sample_directory)\n",
    "                    g_objects.dump(train_sample_directory+'/biasfree_'+str(epoch))\n",
    "                    id_ch = np.random.randint(0, batch_size, 4)\n",
    "                    for i in range(4):\n",
    "                        if g_objects[id_ch[i]].max() > 0.5:\n",
    "                            d.plotVoxelVisdom(np.squeeze(g_objects[id_ch[i]]>0.5), vis, '_'.join(map(str,[epoch,i])))          \n",
    "                if epoch % 50 == 10:\n",
    "                    if not os.path.exists(model_directory):\n",
    "                        os.makedirs(model_directory)      \n",
    "                    saver.save(sess, save_path = model_directory + '/biasfree_' + str(epoch) + '.cptk')\n",
    "\n",
    "\n",
    "def testGAN(trained_model_path=None, n_batches=40):\n",
    "\n",
    "    weights = initialiseWeights()\n",
    "\n",
    "    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n",
    "    net_g_test = generator(z_vector, phase_train=True, reuse=True)\n",
    "\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, trained_model_path) \n",
    "\n",
    "        # output generated chairs\n",
    "        for i in range(n_batches):\n",
    "            next_sigma = float(raw_input())\n",
    "            z_sample = np.random.normal(0, next_sigma, size=[batch_size, z_size]).astype(np.float32)\n",
    "            g_objects = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n",
    "            id_ch = np.random.randint(0, batch_size, 4)\n",
    "            for i in range(4):\n",
    "                print(g_objects[id_ch[i]].max(), g_objects[id_ch[i]].min(), g_objects[id_ch[i]].shape)\n",
    "                if g_objects[id_ch[i]].max() > 0.5:\n",
    "                    d.plotVoxelVisdom(np.squeeze(g_objects[id_ch[i]]>0.5), vis, '_'.join(map(str,[i])))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #test = bool(int(sys.argv[1]))\n",
    "    #if test:\n",
    "    #    path = sys.argv[2]\n",
    "    #    testGAN(trained_model_path=path)\n",
    "    #else:\n",
    "    #    ckpt = sys.argv[2]\n",
    "    if True:\n",
    "            trainGAN(is_dummy=False, checkpoint=None)\n",
    "    else:\n",
    "            trainGAN(is_dummy=False, checkpoint=ckpt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
