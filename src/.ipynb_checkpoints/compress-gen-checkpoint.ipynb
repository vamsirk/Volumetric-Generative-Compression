{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies not loaded, some functionality may not work\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# import visdom\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dataIO as d\n",
    "\n",
    "from tqdm import *\n",
    "from utils import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "z_size = 200\n",
    "cube_len = 32\n",
    "leak_value = 0.2\n",
    "weights = {}\n",
    "l = 0.5\n",
    "lr = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(z, batch_size=batch_size, phase_train=True, reuse=False):\n",
    "\n",
    "    strides    = [1,2,2,2,1]\n",
    "\n",
    "    with tf.variable_scope(\"gen\", reuse=reuse):\n",
    "        g_1 = tf.layers.dense(z, 256*2*2*2, kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n",
    "        g_1 = tf.reshape(g_1, (-1, 2,2,2,256))\n",
    "        g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n",
    "        g_1 = lrelu(g_1)\n",
    "\n",
    "        g_2 = tf.nn.conv3d_transpose(g_1, weights['wg2'], (batch_size,4,4,4,256), strides=strides, padding=\"SAME\")\n",
    "        g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n",
    "        g_2 = lrelu(g_2)\n",
    "\n",
    "        g_3 = tf.nn.conv3d_transpose(g_2, weights['wg3'], (batch_size,8,8,8,128), strides=strides, padding=\"SAME\")\n",
    "        g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n",
    "        g_3 = lrelu(g_3)\n",
    "\n",
    "        g_4 = tf.nn.conv3d_transpose(g_3, weights['wg4'], (batch_size,16,16,16,64), strides=strides, padding=\"SAME\")\n",
    "        g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n",
    "        g_4 = lrelu(g_4)\n",
    "        \n",
    "        g_5 = tf.nn.conv3d_transpose(g_4, weights['wg5'], (batch_size,32,32,32,1), strides=strides, padding=\"SAME\")\n",
    "        g_5 = tf.nn.sigmoid(g_5)\n",
    "\n",
    "    print (g_1, 'g1')\n",
    "    print (g_2, 'g2')\n",
    "    print (g_3, 'g3')\n",
    "    print (g_4, 'g4')\n",
    "    print (g_5, 'g5')\n",
    "    \n",
    "    return g_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(inputs, encoding_size, phase_train=True, reuse=False):\n",
    "\n",
    "    strides    = [1,2,2,2,1]\n",
    "    with tf.variable_scope(\"enc\", reuse=reuse):\n",
    "        xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        we1 = tf.get_variable(\"we1\", shape=[4, 4, 4, 1, 32], initializer=xavier_init)\n",
    "        we2 = tf.get_variable(\"we2\", shape=[4, 4, 4, 32, 64], initializer=xavier_init)\n",
    "        we3 = tf.get_variable(\"we3\", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n",
    "        we4 = tf.get_variable(\"we4\", shape=[4, 4, 4, 128, 256], initializer=xavier_init)    \n",
    "        d_1 = tf.nn.conv3d(inputs, we1, strides=strides, padding=\"SAME\")\n",
    "        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n",
    "        d_1 = lrelu(d_1, leak_value)\n",
    "\n",
    "        d_2 = tf.nn.conv3d(d_1, we2, strides=strides, padding=\"SAME\") \n",
    "        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n",
    "        d_2 = lrelu(d_2, leak_value)\n",
    "        \n",
    "        d_3 = tf.nn.conv3d(d_2, we3, strides=strides, padding=\"SAME\")  \n",
    "        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n",
    "        d_3 = lrelu(d_3, leak_value) \n",
    "\n",
    "        d_4 = tf.nn.conv3d(d_3, we4, strides=strides, padding=\"SAME\")     \n",
    "        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n",
    "        d_4 = lrelu(d_4)\n",
    "\n",
    "        d_5 = tf.contrib.layers.flatten(d_4)\n",
    "        d_5 = tf.layers.dense(d_5, encoding_size, kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        d_5_no_sigmoid = d_5\n",
    "        d_5 = tf.nn.sigmoid(d_5)\n",
    "\n",
    "    print (d_1, 'd1')\n",
    "    print (d_2, 'd2')\n",
    "    print (d_3, 'd3')\n",
    "    print (d_4, 'd4')\n",
    "    print (d_5, 'd5')\n",
    "\n",
    "    return d_5, d_5_no_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseWeights():\n",
    "\n",
    "    global weights\n",
    "    xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "    weights['wg2'] = tf.get_variable(\"wg2\", shape=[4, 4, 4, 256, 256], initializer=xavier_init)\n",
    "    weights['wg3'] = tf.get_variable(\"wg3\", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n",
    "    weights['wg4'] = tf.get_variable(\"wg4\", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n",
    "    weights['wg5'] = tf.get_variable(\"wg5\", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n",
    "   \n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:666: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Tensor(\"enc/Maximum:0\", shape=(100, 16, 16, 16, 32), dtype=float32) d1\n",
      "Tensor(\"enc/Maximum_1:0\", shape=(100, 8, 8, 8, 64), dtype=float32) d2\n",
      "Tensor(\"enc/Maximum_2:0\", shape=(100, 4, 4, 4, 128), dtype=float32) d3\n",
      "Tensor(\"enc/Maximum_3:0\", shape=(100, 2, 2, 2, 256), dtype=float32) d4\n",
      "Tensor(\"enc/Sigmoid:0\", shape=(100, 200), dtype=float32) d5\n",
      "Tensor(\"gen/Maximum:0\", shape=(100, 2, 2, 2, 256), dtype=float32) g1\n",
      "Tensor(\"gen/Maximum_1:0\", shape=(100, 4, 4, 4, 256), dtype=float32) g2\n",
      "Tensor(\"gen/Maximum_2:0\", shape=(100, 8, 8, 8, 128), dtype=float32) g3\n",
      "Tensor(\"gen/Maximum_3:0\", shape=(100, 16, 16, 16, 64), dtype=float32) g4\n",
      "Tensor(\"gen/Sigmoid:0\", shape=(100, 32, 32, 32, 1), dtype=float32) g5\n"
     ]
    }
   ],
   "source": [
    "initialiseWeights()\n",
    "x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n",
    "\n",
    "enc,_ = encoder(x_vector, z_size, phase_train=True, reuse=False) \n",
    "dec = decoder(enc, phase_train=True, reuse=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('./models/chair-final-3/'))\n",
    "#     var = [v for v in tf.trainable_variables()]\n",
    "#     print(sess.eval(var[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a list of variable to load\n",
    "generator_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='gen')\n",
    "generator_vars.extend(list(weights.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/chair-final-3/biasfree_9060.cptk\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(generator_vars)\n",
    "saver.restore(sess, tf.train.latest_checkpoint('./models/chair-final-3/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_pixel = tf.nn.l2_loss(x_vector - dec) / (batch_size*cube_len*cube_len*cube_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     vgg = vgg_network.VGG(vgg_path)\n",
    "#     loss_calculator = LossCalculator(vgg, decoder_net.outputs)\n",
    "#     loss_perceptual = loss_calculator.content_loss(input_img,content_layer='relu4_3',content_weight=1) / batch_size\n",
    "\n",
    "# weighted sum of the two losses\n",
    "loss = l*loss_pixel #+ (1-l)*loss_perceptual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='enc')\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, var_list=encoder_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
