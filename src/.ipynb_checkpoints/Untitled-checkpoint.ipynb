{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies not loaded, some functionality may not work\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import dataIO as d\n",
    "\n",
    "from tqdm import *\n",
    "from utils import *\n",
    "import pickle\n",
    "'''\n",
    "Global Parameters\n",
    "'''\n",
    "n_epochs   = 10000\n",
    "batch_size = 10\n",
    "g_lr       = 0.0025\n",
    "d_lr       = 0.00001\n",
    "beta       = 0.5\n",
    "d_thresh   = 0.8\n",
    "z_size     = 200\n",
    "leak_value = 0.2\n",
    "leak_value1 = 0.2\n",
    "cube_len   = 32\n",
    "obj_ratio  = 0.7\n",
    "obj        = 'chair' \n",
    "\n",
    "train_sample_directory = './train_sample_biasfree_final/'\n",
    "model_directory = './models_biasfree_final/'\n",
    "is_local = False\n",
    "\n",
    "weights, biases = {}, {}\n",
    "\n",
    "\n",
    "experiment_name = 'gan-small-airplane1'\n",
    "\n",
    "train_sample_directory = './train_sample/' + experiment_name + '/'\n",
    "model_directory = './models/' + experiment_name + '/'\n",
    "img_base_directory = './img/'\n",
    "img_directory = img_base_directory + experiment_name + '/'\n",
    "pickle_base_directory = './pickle/'\n",
    "pickle_directory = pickle_base_directory + experiment_name + '/'\n",
    "is_local = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n",
    "\n",
    "    strides    = [1,2,2,2,1]\n",
    "\n",
    "    with tf.variable_scope(\"gen\", reuse=reuse):\n",
    "#         z = tf.reshape(z, (batch_size, 1, 1, 1, z_size))\n",
    "        g_1 = tf.layers.dense(z, 256*2*2*2, kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "        g_1 = tf.reshape(g_1, (-1, 2,2,2,256))\n",
    "#         g_1 = tf.nn.conv3d_transpose(z, weights['wg1'], (batch_size,4,4,4,512), strides=[1,1,1,1,1], padding=\"VALID\")\n",
    "        g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n",
    "        g_1 = tf.nn.relu(g_1)\n",
    "\n",
    "        g_2 = tf.nn.conv3d_transpose(g_1, weights['wg2'], (batch_size,4,4,4,256), strides=strides, padding=\"SAME\")\n",
    "        g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n",
    "        g_2 = tf.nn.relu(g_2)\n",
    "\n",
    "        g_3 = tf.nn.conv3d_transpose(g_2, weights['wg3'], (batch_size,8,8,8,128), strides=strides, padding=\"SAME\")\n",
    "        g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n",
    "        g_3 = tf.nn.relu(g_3)\n",
    "\n",
    "        g_4 = tf.nn.conv3d_transpose(g_3, weights['wg4'], (batch_size,16,16,16,64), strides=strides, padding=\"SAME\")\n",
    "        g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n",
    "        g_4 = tf.nn.relu(g_4)\n",
    "        \n",
    "        g_5 = tf.nn.conv3d_transpose(g_4, weights['wg5'], (batch_size,32,32,32,1), strides=strides, padding=\"SAME\")\n",
    "        g_5 = tf.nn.sigmoid(g_5)\n",
    "#         g_5 = tf.nn.tanh(g_5)\n",
    "\n",
    "    print (g_1, 'g1')\n",
    "    print (g_2, 'g2')\n",
    "    print (g_3, 'g3')\n",
    "    print (g_4, 'g4')\n",
    "    print (g_5, 'g5')\n",
    "    \n",
    "    return g_5\n",
    "\n",
    "\n",
    "def discriminator(inputs, phase_train=True, reuse=False):\n",
    "\n",
    "    strides    = [1,2,2,2,1]\n",
    "    with tf.variable_scope(\"dis\", reuse=reuse):\n",
    "        d_1 = tf.nn.conv3d(inputs, weights['wd1'], strides=strides, padding=\"SAME\")\n",
    "        d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n",
    "        d_1 = lrelu(d_1, leak_value)\n",
    "\n",
    "        d_2 = tf.nn.conv3d(d_1, weights['wd2'], strides=strides, padding=\"SAME\") \n",
    "        d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n",
    "        d_2 = lrelu(d_2, leak_value)\n",
    "        \n",
    "        d_3 = tf.nn.conv3d(d_2, weights['wd3'], strides=strides, padding=\"SAME\")  \n",
    "        d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n",
    "        d_3 = lrelu(d_3, leak_value) \n",
    "\n",
    "        d_4 = tf.nn.conv3d(d_3, weights['wd4'], strides=strides, padding=\"SAME\")     \n",
    "        d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n",
    "        d_4 = lrelu(d_4)\n",
    "\n",
    "        d_5 = tf.contrib.layers.flatten(d_4)\n",
    "        d_5 = tf.layers.dense(d_5, 1, kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         d_5 = tf.nn.conv3d(d_4, weights['wd5'], strides=[1,1,1,1,1], padding=\"VALID\")     \n",
    "        d_5_no_sigmoid = d_5\n",
    "        d_5 = tf.nn.sigmoid(d_5)\n",
    "\n",
    "    print (d_1, 'd1')\n",
    "    print (d_2, 'd2')\n",
    "    print (d_3, 'd3')\n",
    "    print (d_4, 'd4')\n",
    "    print (d_5, 'd5')\n",
    "\n",
    "    return d_5, d_5_no_sigmoid\n",
    "\n",
    "def initialiseWeights():\n",
    "\n",
    "    global weights\n",
    "    xavier_init = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "#     weights['wg1'] = tf.get_variable(\"wg1\", shape=[4, 4, 4, 512, 200], initializer=xavier_init)\n",
    "    weights['wg2'] = tf.get_variable(\"wg2\", shape=[4, 4, 4, 256, 256], initializer=xavier_init)\n",
    "    weights['wg3'] = tf.get_variable(\"wg3\", shape=[4, 4, 4, 128, 256], initializer=xavier_init)\n",
    "    weights['wg4'] = tf.get_variable(\"wg4\", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n",
    "    weights['wg5'] = tf.get_variable(\"wg5\", shape=[4, 4, 4, 1, 64], initializer=xavier_init)    \n",
    "\n",
    "    weights['wd1'] = tf.get_variable(\"wd1\", shape=[4, 4, 4, 1, 32], initializer=xavier_init)\n",
    "    weights['wd2'] = tf.get_variable(\"wd2\", shape=[4, 4, 4, 32, 64], initializer=xavier_init)\n",
    "    weights['wd3'] = tf.get_variable(\"wd3\", shape=[4, 4, 4, 64, 128], initializer=xavier_init)\n",
    "    weights['wd4'] = tf.get_variable(\"wd4\", shape=[4, 4, 4, 128, 256], initializer=xavier_init)    \n",
    "#     weights['wd5'] = tf.get_variable(\"wd5\", shape=[4, 4, 4, 256, 1], initializer=xavier_init)    \n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(is_dummy=False, checkpoint=None):\n",
    "\n",
    "    weights =  initialiseWeights()\n",
    "\n",
    "    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n",
    "    x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n",
    "\n",
    "    net_g_train = generator(z_vector, phase_train=True, reuse=False) \n",
    "\n",
    "    d_output_x, d_no_sigmoid_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n",
    "    d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n",
    "    summary_d_x_hist = tf.summary.histogram(\"d_prob_x\", d_output_x)\n",
    "\n",
    "    d_output_z, d_no_sigmoid_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n",
    "    d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n",
    "    summary_d_z_hist = tf.summary.histogram(\"d_prob_z\", d_output_z)\n",
    "\n",
    "    # Compute the discriminator accuracy\n",
    "    n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n",
    "    n_p_z = tf.reduce_sum(tf.cast(d_output_z < 0.5, tf.int32))\n",
    "    d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n",
    "\n",
    "    # Compute the discriminator and generator loss\n",
    "   # d_loss = -tf.reduce_mean(tf.log(d_output_x) + tf.log(1-d_output_z))\n",
    "  #  g_loss = -tf.reduce_mean(tf.log(d_output_z))\n",
    "\n",
    "    d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_x, labels=tf.ones_like(d_output_x)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.zeros_like(d_output_z)))\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.ones_like(d_output_z)))\n",
    "    \n",
    "#     d_loss = tf.reduce_mean(d_loss)\n",
    "#     g_loss = tf.reduce_mean(g_loss)\n",
    "\n",
    "    summary_d_loss = tf.summary.scalar(\"d_loss\", d_loss)\n",
    "    summary_g_loss = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "    summary_n_p_z = tf.summary.scalar(\"n_p_z\", n_p_z)\n",
    "    summary_n_p_x = tf.summary.scalar(\"n_p_x\", n_p_x)\n",
    "    summary_d_acc = tf.summary.scalar(\"d_acc\", d_acc)\n",
    "\n",
    "    net_g_test = generator(z_vector, phase_train=False, reuse=True)\n",
    "\n",
    "    \n",
    "\n",
    "        # only update the weights for the discriminator network\n",
    "    para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in ['wg', 'bg', 'cris','gen'])]\n",
    "    para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in ['wd', 'bd', 'dis'])]\n",
    "    optimizer_op_d = tf.train.AdamOptimizer(learning_rate=d_lr,beta1=beta).minimize(d_loss,var_list=para_d)\n",
    "        # only update the weights for the generator network\n",
    "    optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss,var_list=para_g)\n",
    "\n",
    "    saver = tf.train.Saver() \n",
    "#     vis = visdom.Visdom()\n",
    "\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    with tf.Session() as sess:  \n",
    "      \n",
    "        sess.run(tf.global_variables_initializer())        \n",
    "        if checkpoint is not None:\n",
    "            saver.restore(sess, checkpoint)        \n",
    "\n",
    "        if is_dummy:\n",
    "            volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n",
    "            print ('Using Dummy Data')\n",
    "        else:\n",
    "            volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio, cube_len=32)\n",
    "            print ('Using ' + obj + ' Data')\n",
    "        volumes = volumes[...,np.newaxis].astype(np.float)\n",
    "    \n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            sess.run(tf.global_variables_initializer())        \n",
    "            if checkpoint is not None:\n",
    "                saver.restore(sess, checkpoint)        \n",
    "\n",
    "            idx = np.random.randint(len(volumes), size=batch_size)\n",
    "            x = volumes[idx]\n",
    "            z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n",
    "            z = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n",
    "            # Update the discriminator and generator\n",
    "            d_summary_merge = tf.summary.merge([summary_d_loss,\n",
    "                                                summary_d_x_hist, \n",
    "                                                summary_d_z_hist,\n",
    "                                                summary_n_p_x,\n",
    "                                                summary_n_p_z,\n",
    "                                                summary_d_acc])\n",
    "\n",
    "            summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n",
    "            summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z})  \n",
    "            d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n",
    "            print (n_x, n_z)\n",
    "            \n",
    "            d_losses.append(discriminator_loss)\n",
    "            g_losses.append(generator_loss)\n",
    "\n",
    "            if d_accuracy < d_thresh:\n",
    "                sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x})\n",
    "                print ('Discriminator Training ', \"epoch: \",epoch,', d_loss:',discriminator_loss,'g_loss:',generator_loss, \"d_acc: \", d_accuracy)\n",
    "\n",
    "            sess.run([optimizer_op_g],feed_dict={z_vector:z})\n",
    "            print ('Generator Training ', \"epoch: \",epoch,', d_loss:',discriminator_loss,'g_loss:',generator_loss, \"d_acc: \", d_accuracy)\n",
    "\n",
    "            # output generated chairs\n",
    "            if epoch % 200 == 0:\n",
    "                g_objects = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n",
    "                if not os.path.exists(train_sample_directory):\n",
    "                    os.makedirs(train_sample_directory)\n",
    "                    \n",
    "                if not os.path.exists(img_directory):\n",
    "                    os.makedirs(img_directory)\n",
    "                    \n",
    "                if not os.path.exists(pickle_directory):\n",
    "                    os.makedirs(pickle_directory)\n",
    "                    \n",
    "                g_objects.dump(train_sample_directory+'/biasfree_'+str(epoch))\n",
    "                id_ch = np.random.randint(0, batch_size, 4)\n",
    "                \n",
    "                with open(pickle_directory + 'd_loss.pickle', 'wb') as file:\n",
    "                    pickle.dump(d_losses, file)\n",
    "                with open(pickle_directory + 'g_loss.pickle', 'wb') as file:\n",
    "                    pickle.dump(g_losses, file)\n",
    "                \n",
    "                for i in range(4):\n",
    "                    if g_objects[id_ch[i]].max() > 0.5:\n",
    "                        objects = np.squeeze(g_objects[id_ch[i]]>0.5)\n",
    "                        d.plotFromVoxels(objects, img_directory+'{}_{}.png'.format(str(epoch), str(i)))\n",
    "            if epoch % 50 == 10:\n",
    "                if not os.path.exists(model_directory):\n",
    "                    os.makedirs(model_directory)      \n",
    "                saver.save(sess, save_path = model_directory + '/biasfree_' + str(epoch) + '.cptk')\n",
    "\n",
    "\n",
    "def testGAN(trained_model_path=None, n_batches=40):\n",
    "\n",
    "    weights = initialiseWeights()\n",
    "\n",
    "    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n",
    "    net_g_test = generator(z_vector, phase_train=True, reuse=True)\n",
    "\n",
    "#     vis = visdom.Visdom()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, trained_model_path) \n",
    "\n",
    "        # output generated chairs\n",
    "        for i in range(n_batches):\n",
    "            next_sigma = float(raw_input())\n",
    "            z_sample = np.random.normal(0, next_sigma, size=[batch_size, z_size]).astype(np.float32)\n",
    "            g_objects = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n",
    "            id_ch = np.random.randint(0, batch_size, 4)\n",
    "            for i in range(4):\n",
    "                print( g_objects[id_ch[i]].max(), g_objects[id_ch[i]].min(), g_objects[id_ch[i]].shape)\n",
    "                if g_objects[id_ch[i]].max() > 0.5:\n",
    "                    pass\n",
    "#                     d.plotVoxelVisdom(np.squeeze(g_objects[id_ch[i]]>0.5), vis, '_'.join(map(str,[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gen/Relu:0\", shape=(10, 2, 2, 2, 256), dtype=float32) g1\n",
      "Tensor(\"gen/Relu_1:0\", shape=(10, 4, 4, 4, 256), dtype=float32) g2\n",
      "Tensor(\"gen/Relu_2:0\", shape=(10, 8, 8, 8, 128), dtype=float32) g3\n",
      "Tensor(\"gen/Relu_3:0\", shape=(10, 16, 16, 16, 64), dtype=float32) g4\n",
      "Tensor(\"gen/Sigmoid:0\", shape=(10, 32, 32, 32, 1), dtype=float32) g5\n",
      "Tensor(\"dis/Maximum:0\", shape=(10, 16, 16, 16, 32), dtype=float32) d1\n",
      "Tensor(\"dis/Maximum_1:0\", shape=(10, 8, 8, 8, 64), dtype=float32) d2\n",
      "Tensor(\"dis/Maximum_2:0\", shape=(10, 4, 4, 4, 128), dtype=float32) d3\n",
      "Tensor(\"dis/Maximum_3:0\", shape=(10, 2, 2, 2, 256), dtype=float32) d4\n",
      "Tensor(\"dis/Sigmoid:0\", shape=(10, 1), dtype=float32) d5\n",
      "Tensor(\"dis_1/Maximum:0\", shape=(10, 16, 16, 16, 32), dtype=float32) d1\n",
      "Tensor(\"dis_1/Maximum_1:0\", shape=(10, 8, 8, 8, 64), dtype=float32) d2\n",
      "Tensor(\"dis_1/Maximum_2:0\", shape=(10, 4, 4, 4, 128), dtype=float32) d3\n",
      "Tensor(\"dis_1/Maximum_3:0\", shape=(10, 2, 2, 2, 256), dtype=float32) d4\n",
      "Tensor(\"dis_1/Sigmoid:0\", shape=(10, 1), dtype=float32) d5\n",
      "Tensor(\"gen_1/Relu:0\", shape=(10, 2, 2, 2, 256), dtype=float32) g1\n",
      "Tensor(\"gen_1/Relu_1:0\", shape=(10, 4, 4, 4, 256), dtype=float32) g2\n",
      "Tensor(\"gen_1/Relu_2:0\", shape=(10, 8, 8, 8, 128), dtype=float32) g3\n",
      "Tensor(\"gen_1/Relu_3:0\", shape=(10, 16, 16, 16, 64), dtype=float32) g4\n",
      "Tensor(\"gen_1/Sigmoid:0\", shape=(10, 32, 32, 32, 1), dtype=float32) g5\n",
      "INFO:tensorflow:Restoring parameters from /home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001\n"
     ]
    },
    {
     "ename": "DataLossError",
     "evalue": "Unable to open table file /home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[Node: save/RestoreV2_23 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_23/tensor_names, save/RestoreV2_23/shape_and_slices)]]\n\t [[Node: save/RestoreV2_65/_145 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_308_save/RestoreV2_65\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'save/RestoreV2_23', defined at:\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-938df3e92f38>\", line 2, in <module>\n    trainGAN(is_dummy=False, checkpoint='/home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001')\n  File \"<ipython-input-2-94556169a1f0>\", line 50, in trainGAN\n    saver = tf.train.Saver()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): Unable to open table file /home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[Node: save/RestoreV2_23 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_23/tensor_names, save/RestoreV2_23/shape_and_slices)]]\n\t [[Node: save/RestoreV2_65/_145 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_308_save/RestoreV2_65\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Unable to open table file /home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[Node: save/RestoreV2_23 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_23/tensor_names, save/RestoreV2_23/shape_and_slices)]]\n\t [[Node: save/RestoreV2_65/_145 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_308_save/RestoreV2_65\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDataLossError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-938df3e92f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mtrainGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_dummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-94556169a1f0>\u001b[0m in \u001b[0;36mtrainGAN\u001b[0;34m(is_dummy, checkpoint)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dummy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1546\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1548\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataLossError\u001b[0m: Unable to open table file /home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[Node: save/RestoreV2_23 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_23/tensor_names, save/RestoreV2_23/shape_and_slices)]]\n\t [[Node: save/RestoreV2_65/_145 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_308_save/RestoreV2_65\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'save/RestoreV2_23', defined at:\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-938df3e92f38>\", line 2, in <module>\n    trainGAN(is_dummy=False, checkpoint='/home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001')\n  File \"<ipython-input-2-94556169a1f0>\", line 50, in trainGAN\n    saver = tf.train.Saver()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\n    self.build()\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/vamsi/miniconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n\nDataLossError (see above for traceback): Unable to open table file /home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk.data-00000-of-00001: Data loss: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n\t [[Node: save/RestoreV2_23 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_arg_save/Const_0_0, save/RestoreV2_23/tensor_names, save/RestoreV2_23/shape_and_slices)]]\n\t [[Node: save/RestoreV2_65/_145 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_308_save/RestoreV2_65\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "        trainGAN(is_dummy=False, checkpoint='/home/vamsi/Downloads/tf-3dgan-master/src/models/gan-small-airplane/biasfree_7810.cptk')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, batch_size=batch_size, phase_train=True, reuse=False):\n",
    "\n",
    "    with tf.device('/device:GPU:2'):\n",
    "        strides    = [2,2,2]\n",
    "\n",
    "        with tf.variable_scope(\"gen\", reuse=reuse):\n",
    "            g_1 = tf.layers.dense(z, 256*2*2*2, kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "            g_1 = tf.reshape(g_1, (-1, 2,2,2,256))\n",
    "            g_1 = tf.contrib.layers.batch_norm(g_1, is_training=phase_train)\n",
    "            g_1 = tf.nn.relu(g_1)\n",
    "\n",
    "\n",
    "            g_2 = tf.layers.conv3d_transpose(g_1, 256, (4,4,4), strides=strides, padding=\"SAME\")\n",
    "            #g_2 = tf.nn.bias_add(g_2, biases['bg2'])\n",
    "            g_2 = tf.contrib.layers.batch_norm(g_2, is_training=phase_train)\n",
    "            g_2 = lrelu(g_2, leak_value)\n",
    "\n",
    "            g_3 = tf.layers.conv3d_transpose(g_2, 128, (4,4,4), strides=strides, padding=\"SAME\")\n",
    "            #g_3 = tf.nn.bias_add(g_3, biases['bg3'])\n",
    "            g_3 = tf.contrib.layers.batch_norm(g_3, is_training=phase_train)\n",
    "            g_3 = lrelu(g_3, leak_value)\n",
    "\n",
    "            g_4 = tf.layers.conv3d_transpose(g_3, 64, (4,4,4), strides=strides, padding=\"SAME\")\n",
    "            #g_4 = tf.nn.bias_add(g_4, biases['bg4'])\n",
    "            g_4 = tf.contrib.layers.batch_norm(g_4, is_training=phase_train)\n",
    "            g_4 = lrelu(g_4, leak_value)\n",
    "\n",
    "            g_5 = tf.layers.conv3d_transpose(g_4, 1, (4,4,4), strides=strides, padding=\"SAME\")\n",
    "            #g_5 = tf.nn.bias_add(g_5, biases['bg5'])\n",
    "            #g_5 = tf.contrib.layers.batch_norm(g_5, is_training=phase_train)    \n",
    "            g_5 = tf.nn.sigmoid(g_5)\n",
    "\n",
    "        print(g_1, 'g1')\n",
    "        print(g_2, 'g2')\n",
    "        print(g_3, 'g3')\n",
    "        print(g_4, 'g4')\n",
    "        print(g_5, 'g5')\n",
    "\n",
    "        return g_5\n",
    "\n",
    "\n",
    "def discriminator(inputs, phase_train=True, reuse=False):\n",
    "    \n",
    "    with tf.device('/device:GPU:2'):\n",
    "\n",
    "        strides    = [2,2,2]\n",
    "        with tf.variable_scope(\"dis\", reuse=reuse):\n",
    "            \n",
    "            d_1 = tf.layers.conv3d(inputs, 32, (4,4,4), strides=strides, padding=\"SAME\")\n",
    "            #d_1 = tf.nn.bias_add(d_1, biases['bd1'])\n",
    "            d_1 = tf.contrib.layers.batch_norm(d_1, is_training=phase_train)                               \n",
    "            d_1 = lrelu(d_1, leak_value1)\n",
    "\n",
    "            d_2 = tf.layers.conv3d(d_1,  64, (4,4,4), strides=strides, padding=\"SAME\") \n",
    "            #d_2 = tf.nn.bias_add(d_2, biases['bd2'])\n",
    "            d_2 = tf.contrib.layers.batch_norm(d_2, is_training=phase_train)\n",
    "            d_2 = lrelu(d_2, leak_value1)\n",
    "\n",
    "            d_3 = tf.layers.conv3d(d_2,  128, (4,4,4), strides=strides, padding=\"SAME\")  \n",
    "            #d_3 = tf.nn.bias_add(d_3, biases['bd3'])\n",
    "            d_3 = tf.contrib.layers.batch_norm(d_3, is_training=phase_train)\n",
    "            d_3 = lrelu(d_3, leak_value1) \n",
    "\n",
    "            d_4 = tf.layers.conv3d(d_3, 256, (4,4,4), strides=strides, padding=\"SAME\")     \n",
    "            #d_4 = tf.nn.conv3d(d_3, weights['wd4'], strides=strides, padding=\"SAME\")\n",
    "            #d_4 = tf.nn.bias_add(d_4, biases['bd4'])\n",
    "            d_4 = tf.contrib.layers.batch_norm(d_4, is_training=phase_train)\n",
    "            d_4 = lrelu(d_4, leak_value1)\n",
    "\n",
    "            d_5 = tf.contrib.layers.flatten(d_4)\n",
    "            d_5 = tf.layers.dense(d_5, 1, kernel_initializer=tf.random_normal_initializer(stddev=0.02))\n",
    "#         d_5 = tf.nn.conv3d(d_4, weights['wd5'], strides=[1,1,1,1,1], padding=\"VALID\")     \n",
    "            d_5_no_sigmoid = d_5\n",
    "            d_5 = tf.nn.sigmoid(d_5)\n",
    "\n",
    "        print(d_1, 'd1')\n",
    "        print(d_2, 'd2')\n",
    "        print(d_3, 'd3')\n",
    "        print(d_4, 'd4')\n",
    "        print(d_5, 'd5')\n",
    "\n",
    "        return d_5, d_5_no_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(is_dummy=False):\n",
    "    \n",
    "    with tf.device('/device:GPU:2'):\n",
    "        cube_len = 32\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        dis_lr = tf.Variable(0.0005, name='d_lr')\n",
    "        z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n",
    "        x_vector = tf.placeholder(shape=[batch_size,cube_len,cube_len,cube_len,1],dtype=tf.float32) \n",
    "\n",
    "        net_g_train = generator(z_vector, phase_train=True, reuse=False) \n",
    "\n",
    "        d_output_x, d_no_sigmoid_output_x = discriminator(x_vector, phase_train=True, reuse=False)\n",
    "        d_output_x = tf.maximum(tf.minimum(d_output_x, 0.99), 0.01)\n",
    "        summary_d_x_hist = tf.summary.histogram(\"d_prob_x\", d_output_x)\n",
    "\n",
    "        d_output_z, d_no_sigmoid_output_z = discriminator(net_g_train, phase_train=True, reuse=True)\n",
    "        d_output_z = tf.maximum(tf.minimum(d_output_z, 0.99), 0.01)\n",
    "        summary_d_z_hist = tf.summary.histogram(\"d_prob_z\", d_output_z)\n",
    "\n",
    "        # Compute the discriminator accuracy\n",
    "        n_p_x = tf.reduce_sum(tf.cast(d_output_x > 0.5, tf.int32))\n",
    "        n_p_z = tf.reduce_sum(tf.cast(d_output_z <= 0.5, tf.int32))\n",
    "        d_acc = tf.divide(n_p_x + n_p_z, 2 * batch_size)\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        d_labels = tf.random_uniform( tf.shape(d_output_x), minval=0.7, maxval=1.2)\n",
    "        #g_labels = tf.random_uniform( tf.shape(d_output_x), minval=0.0, maxval=0.3)\n",
    "        # Compute the discriminator and generator loss\n",
    "        d_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_x, labels=tf.ones_like(d_output_x)) + tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.zeros_like(d_output_z)))\n",
    "        g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_no_sigmoid_output_z, labels=tf.ones_like(d_output_z)))\n",
    "    \n",
    "\n",
    "        summary_d_loss = tf.summary.scalar(\"d_loss\", d_loss)\n",
    "        summary_g_loss = tf.summary.scalar(\"g_loss\", g_loss)\n",
    "        summary_n_p_z = tf.summary.scalar(\"n_p_z\", n_p_z)\n",
    "        summary_n_p_x = tf.summary.scalar(\"n_p_x\", n_p_x)\n",
    "        summary_d_acc = tf.summary.scalar(\"d_acc\", d_acc)\n",
    "\n",
    "        net_g_test = generator(z_vector, phase_train=False, reuse=True)\n",
    "\n",
    "        #para_g = [var for var in tf.trainable_variables() if any(x in var.name for x in ['wg', 'gen'])]\n",
    "        #para_d = [var for var in tf.trainable_variables() if any(x in var.name for x in ['wd', 'dis'])]\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            optimizer_op_d = tf.train.AdamOptimizer(learning_rate=dis_lr,beta1=beta).minimize(d_loss)\n",
    "            # only update the weights for the generator network\n",
    "            optimizer_op_g = tf.train.AdamOptimizer(learning_rate=g_lr,beta1=beta).minimize(g_loss)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=50) \n",
    "        \n",
    "        config=tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "        with tf.Session(config=config) as sess:  \n",
    "\n",
    "            sess.run(tf.global_variables_initializer())        \n",
    "            z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n",
    "            x_sample = np.random.normal(0, 0.33, size=[batch_size, cube_len,cube_len,cube_len,1]).astype(np.float32)\n",
    "            if is_dummy:\n",
    "                volumes = np.random.randint(0,2,(batch_size,cube_len,cube_len,cube_len))\n",
    "                print('Using Dummy Data')\n",
    "            else:\n",
    "                volumes = d.getAll(obj=obj, train=True, is_local=is_local, obj_ratio=obj_ratio,cube_len=32)\n",
    "                print('Using ' + obj + ' Data')\n",
    "            volumes = volumes[...,np.newaxis].astype(np.float) \n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                #d_lr = tf.cond( tf.divide(n_p_z, batch_size)>0.3, lambda: np.float32(0.00001), lambda: np.float32(0.00005))\n",
    "                \n",
    "                idx = np.random.randint(len(volumes), size=batch_size)\n",
    "                x = volumes[idx]\n",
    "                z = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n",
    "\n",
    "                # Update the discriminator and generator\n",
    "                d_summary_merge = tf.summary.merge([summary_d_loss,\n",
    "                                                    summary_d_x_hist, \n",
    "                                                    summary_d_z_hist,\n",
    "                                                    summary_n_p_x,\n",
    "                                                    summary_n_p_z,\n",
    "                                                    summary_d_acc])\n",
    "\n",
    "                summary_d, discriminator_loss = sess.run([d_summary_merge,d_loss],feed_dict={z_vector:z, x_vector:x})\n",
    "                summary_g, generator_loss = sess.run([summary_g_loss,g_loss],feed_dict={z_vector:z})  \n",
    "                d_accuracy, n_x, n_z = sess.run([d_acc, n_p_x, n_p_z],feed_dict={z_vector:z, x_vector:x})\n",
    "                print(n_x, n_z)\n",
    "                \n",
    "                if n_z<10:\n",
    "                        print('d_lr')\n",
    "                        d_lr = 0.0001\n",
    "                else:\n",
    "                        d_lr = 0.0001\n",
    "    \n",
    "                if d_accuracy < d_thresh:\n",
    "                    sess.run([optimizer_op_d],feed_dict={z_vector:z, x_vector:x, dis_lr: d_lr})\n",
    "                    print('Discriminator Training ', \"epoch: \",epoch,', d_loss:',discriminator_loss,'g_loss:',generator_loss, \"d_acc: \", d_accuracy)\n",
    "\n",
    "                #sess.run([optimizer_op_g],feed_dict={z_vector:z})\n",
    "                #print('Generator Training ', \"epoch: \",epoch,', d_loss:',discriminator_loss,'g_loss:',generator_loss, \"d_acc: \", d_accuracy)\n",
    "\n",
    "                # output generated chairs\n",
    "                if epoch % 100 == 10:\n",
    "                    g_chairs = sess.run(net_g_test,feed_dict={z_vector:z_sample,x_vector:x_sample})\n",
    "                    if not os.path.exists(train_sample_directory):\n",
    "                        os.makedirs(train_sample_directory)\n",
    "                    g_chairs.dump(train_sample_directory+'/'+str(epoch))\n",
    "\n",
    "                if epoch % 100 == 10:\n",
    "                    if not os.path.exists(model_directory):\n",
    "                        os.makedirs(model_directory)      \n",
    "                    saver.save(sess, save_path = model_directory + '/' + str(epoch) + '.cptk')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #is_dummy = bool(int(sys.argv[1]))\n",
    "    trainGAN(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import visdom\n",
    "def testGAN(trained_model_path=None, n_batches=40):\n",
    "\n",
    "    #weights = initialiseWeights()\n",
    "    #global weights\n",
    "    z_vector = tf.placeholder(shape=[batch_size,z_size],dtype=tf.float32) \n",
    "    net_g_test = generator(z_vector, phase_train=True, reuse=True)\n",
    "\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    config=tf.ConfigProto(allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, trained_model_path) \n",
    "\n",
    "        # output generated chairs\n",
    "        for i in range(n_batches):\n",
    "            #next_sigma = float(raw_input())\n",
    "            z_sample = np.random.normal(0, 0.33, size=[batch_size, z_size]).astype(np.float32)\n",
    "            g_objects = sess.run(net_g_test,feed_dict={z_vector:z_sample})\n",
    "            id_ch = np.random.randint(0, batch_size, 4)\n",
    "            for i in range(4):\n",
    "                print(g_objects[id_ch[i]].max(), g_objects[id_ch[i]].min(), g_objects[id_ch[i]].shape)\n",
    "                if g_objects[id_ch[i]].max() > 0.5:\n",
    "                    d.plotVoxelVisdom(np.squeeze(g_objects[id_ch[i]]>0.5), vis, '_'.join(map(str,[i])))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "        testGAN(trained_model_path='/home/vamsi/Downloads/tf-3dgan-master/src/models/3010.cptk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pickle\n",
    "\n",
    "def plotFromVoxels(voxels):\n",
    "    z,x,y = voxels.nonzero()\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, z, zdir='z', c= 'red')\n",
    "    plt.savefig('test.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "with open('train_sample/3010', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "plotFromVoxels(model[1].squeeze()>0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
